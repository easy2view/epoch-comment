### 【名家专栏】您的人工智能觉醒了吗？

---

#### [首页](../../../..?n13238848) &nbsp;|&nbsp; [评论](../../../../../epoch-comment?n13238848) &nbsp;|&nbsp; [专题](../../../../../epoch-special?n13238848) &nbsp;|&nbsp; [禁闻](../../../../../epoch-news?n13238848) &nbsp;|&nbsp; [禁书](../../../../../books?n13238848) &nbsp;|&nbsp; [翻墙](https://github.com/gfw-breaker/nogfw/blob/master/README.md?n13238848)


<div><img alt="【名家专栏】您的人工智能觉醒了吗？" class="attachment-djy_600_400 size-djy_600_400 wp-post-image" src="https://i.epochtimes.com/assets/uploads/2021/09/id13238917-facial-recognition-700x420-600x400.jpg"/>
<div class="caption">
 2019年1月10日，在拉斯维加斯的一个展览上，现场演示在密集人群使用人工智能和面部识别的时空技术。(David McNew/AFP via Getty Images)
</div></div><hr/><div class="post_content" id="artbody" itemprop="articleBody">
 <!-- article content begin -->
 <p>
  【大纪元2021年09月16日讯】（英文大纪元专栏作家Mark Stamp撰文／信宇编译）就核心而言，
  <ok href="https://www.epochtimes.com/gb/tag/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD.html">
   人工智能
  </ok>
  （artificial intelligence，简称AI）就是统计性歧视；也就是说，人工智能算法从统计信息中提取决策见解。与普通的统计性歧视技术相比，人工智能算法能够通过一个涉及数据训练的过程来“学习”。
 </p>
 <p>
  <ok href="https://www.epochtimes.com/gb/tag/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD.html">
   人工智能
  </ok>
  成功的关键就是，算法能够从训练数据中归纳，而不是简单地记忆信息。
 </p>
 <p>
  人类对人工智能的探索并不新鲜，因为有关人工神经元的初始工作可以追溯至20世纪40年代，而50年代开发的一些基本模型至今仍在使用。然而，人工智能在各个领域的推广应用是一个最近不断兴起的发展趋势，而这个趋势在未来数年里可能会成倍暴涨。
 </p>
 <p>
  既然人工智能拥有如此悠久的历史，为什么它最近才突然爆红呢？在此冒昧借用美国歌手安德里亚‧特鲁（Andrea True）曾经发行的音乐专辑名称来回答，答案就是“更多、更多、更多”（more, more, more）。具体而言，就是我们拥有更多的计算能力和更多的数据，因而我们能够建立具有更多人工神经元层的模型。这种“深度学习”方法已经产生了更强大和更高效的模型，这在以往是不可想像的。
 </p>
 <p>
  关于人工智能存在偏见的说法在今天相当普遍。有鉴于此，学术界和工业界正在兴起一场运动，通过建立包含各种公平概念的人工智能系统，以根除某些类型的偏见。然而，在我看来，这种努力可能会阻碍人工智能在许多应用领域的发展，而最糟糕的结果就是，此举可能会把人工智能沦为一门伪科学。
 </p>
 <p>
  “废料进，废品出。”这句古训当然也适用于人工智能。如果用于训练人工智能模型的数据存在偏见，那么由此产生的模型将会忠实地再现这种偏见。
 </p>
 <p>
  与用于训练的数据相比，人工智能算法不存在固有的偏见；因为无论训练数据如何，它都追求相同的学习策略。因此，似乎显而易见的是，指控人工智能存在偏见，其实旨在获得更好的数据以提升人工智能模型。
 </p>
 <p>
  然而事实并非如此。当前针对人工智能公平性的研究主要集中于构建模型，无论训练数据如何，都不会产生特定的结果。此举可能导致未来对训练数据进行修改，或对人工智能训练算法的内部运作进行修补。
 </p>
 <p>
  无论出现何种情形，目的都是防止最终模型产生某些不良结果，无论数据可能呈现模型何种信息。
 </p>
 <p>
  假设我们收集了面向海量个体的各种统计数据（如身高、体重、鞋码，等等）组成的数据库。此外，再假设我们希望确保这个模型不会歧视高个子人群。那么我们可以轻易地忽略数据库中的身高信息，从而阻止整个人工智能模型直接使用身高作为辨别特征。
 </p>
 <p>
  然而，鞋子大小和体重可能会间接表明身高，导致身高仍然是人工智能决策过程中的一个因素。因此，简单地从训练数据中剔除一个特征，这样的粗暴行为可能不足以防止特定的偏见渗入受训的人工智能模型。
 </p>
 <p>
  另一种方法是修改训练过程本身，这也是任何人工智能算法的核心所在。根据特定人工智能技术的具体情况，有多种方法可以用于修改训练算法，以使模型不会根据训练样本中的受试者身高进行身份歧视。
 </p>
 <p>
  无论是修改数据还是更改算法，我们都人为地限制了人工智能模型的可用信息。更改算法可能还是更直接、更有效的方法。
 </p>
 <p>
  今天主导人工智能的深度学习算法以不透明著称于世；也就是说，人们很难理解模型是如何做出决策的。通过从根本上改变这些模型以消除所谓的不良结果，我们打开了操纵的潘多拉魔盒，这些操纵将有意或无意地代入设计者的种种偏见。
 </p>
 <p>
  统计性歧视是人工智能的核心，人工智能模型仍然会根据训练数据的某些信息进行自然歧视。而且，由于这些模型具有不透明性，事后要找出代入偏见的来源往往是几乎不可能的。
 </p>
 <p>
  基于这样的公平原则，不难想像旨在检测诸如“假新闻”或“仇恨言论”等模糊概念的人工智能模型，其构建方式难免会偏向政治光谱的某一方。这样的模型将为其（带有偏见的）结果添加一层科学体面的外衣，而且很难发现任何内在偏见的来源和程度。
 </p>
 <p>
  尽管其研究目的可能精神可嘉，研究问题也确实有趣，然而人工智能的公平性问题大大增加了搬弄是非的可能性。最终，这些类型的操纵将威胁和损害外界对人工智能的信任。
 </p>
 <p>
  已故纽约参议员丹尼尔‧帕特里克‧莫尼汉（Daniel Patrick Moynihan）曾说过一句名言：“每个人都有权发表自己的意见，但没有权利发表自己的事实。”
 </p>
 <p>
  包括“公平”在内的人工智能模型带来了将事实和意见纠缠在一起的前景，这可能会使两者几乎无法分开。在这种情况下，人工智能开发者的意见可能会被提升为客观“科学”的“事实”，至少那些赞同开发者观点的人会对此推波助澜。
 </p>
 <p>
  与之相对，那些不同意开发者意见的人士就有理由相信，人工智能可能被操纵，只是为了产生一个预先设定的结果。
 </p>
 <p>
  <em>
   原文：
   <ok href="https://www.theepochtimes.com/is-your-ai-woke_3988539.html">
    Is Your AI Woke?
   </ok>
   刊登于英文《大纪元时报》。
  </em>
 </p>
 <p>
  <em>
   作者简介：
  </em>
 </p>
 <p>
  <em>
   马克‧斯坦普（Mark Stamp）是加州圣何塞州立大学（San Jose State University）的计算机科学教授。他的教学和研究领域主要集中于信息安全和机器学习等方面。他在涉及信息安全和机器学习等领域的各种主题上发表了超过125篇研究性论文，并撰写了一系列备受好评的教科书，如《信息安全：原则与实践》（Information Security: Principles and Practices，由Wiley出版）和《机器学习与信息安全应用入门》（Introduction to Machine Learning with Applications in Information Security，由Chapman and Hall/CRC出版）等。
  </em>
 </p>
 <p>
  <em>
   本文仅代表作者观点，并不一定反映《大纪元时报》立场。
  </em>
 </p>
 <p>
  责任编辑：高静#
 </p>
 <!-- article content end -->
 <div id="below_article_ad">
 </div>
</div>


---

原文链接（需翻墙）：https://www.epochtimes.com/gb/21/9/16/n13238848.htm